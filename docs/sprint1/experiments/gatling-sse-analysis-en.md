# üöÄ Analysis: Feature Request for Gatling?

**Date**: October 22, 2025
**Context**: TTFT vs P99 inconsistency analysis (Sprint 1)
**Question**: Should Gatling support native end-to-end latency measurement for SSE?

---

## üìã Executive Summary

**User Question:**
> "Could this observation be a new feature for the Gatling team to improve the tool? I understand the official documentation may be confusing, but does it actually confuse response time vs what the SSE protocol delivers?"

**Short Answer:** ‚úÖ **YES**, this is a valid and useful feature request for the community.

**Important Nuance:** Gatling does NOT confuse the SSE protocol - its behavior is correct according to the HTTP standard. However, the **user expectation** (measuring perceived latency) is legitimate and currently not covered.

---

## üîç Two-Perspective Analysis

### Perspective 1: Gatling is Right ‚úÖ

**Argument:**
- The HTTP protocol defines that a request "completes" when the connection is established
- SSE is simply a `Content-Type: text/event-stream` over HTTP
- RFC 6202 (SSE) does NOT define a standard "completion marker"
- Gatling perfectly complies with the HTTP standard

**Evidence:**
```
POST /v1/chat/completions HTTP/1.1
Host: api.openai.com
Content-Type: application/json
...

HTTP/1.1 200 OK  ‚Üê Here Gatling marks "request complete"
Content-Type: text/event-stream
Transfer-Encoding: chunked

data: {"choices":[...]}  ‚Üê This is no longer part of the "request"
...
data: [DONE]
```

**From this perspective:**
- ‚úÖ Gatling correctly measures the HTTP request/response
- ‚úÖ Streaming is a "post-request event"
- ‚úÖ Correct behavior according to RFC

---

### Perspective 2: The User is Right ‚úÖ

**Argument:**
- In LLM applications, **perceived latency** is what matters
- A request is not "useful" until the stream completes
- The user experience includes ALL streaming
- Gatling should offer this option

**Evidence:**
```
User: "What is the capital of France?"
System: HTTP 200 OK in 558ms  ‚Üê Gatling says "done"
User: [Waiting...]
System: "Paris" appears on screen after 2,018ms  ‚Üê User says "done"
```

**From this perspective:**
- ‚ö†Ô∏è Gatling P99 = 558ms does NOT represent user reality
- ‚ö†Ô∏è For UX SLAs, we need to measure until `[DONE]`
- ‚ö†Ô∏è Gatling's metric can lead to incorrect optimizations

---

## üéØ Conclusion: Both Are Right

### Gatling is not "wrong" - it's designed for classic HTTP

**Traditional HTTP Request/Response:**
```
Client: GET /api/data
Server: 200 OK + JSON body
         ‚Üë
    Measuring latency here makes perfect sense
```

**SSE/Streaming (LLM case):**
```
Client: POST /completions
Server: 200 OK
         ‚Üë
    Gatling measures here... but the real value comes later
Server: [streaming for 2+ seconds]
Server: [DONE]
         ‚Üë
    Here is where the user perceives "completed"
```

---

## üí° Feature Request Proposal

### Proposed API

```java
// Current behavior (default)
sse("Connect to LLM")
  .post("/v1/chat/completions")
  .body(StringBody(requestBody))
  // Measures only until HTTP 200 OK

// New behavior (opt-in)
sse("Connect to LLM")
  .post("/v1/chat/completions")
  .body(StringBody(requestBody))
  .measureUntilStreamCompletion()  // ‚Üê NEW
  .completionMarker("[DONE]")      // ‚Üê NEW (optional)
  // Now measures until receiving [DONE]
```

### Expected Behavior

**With `.measureUntilStreamCompletion()`:**
- Gatling's timer does NOT stop at HTTP 200
- Continues measuring during the `.asLongAs()` loop
- Stops when completion marker is detected
- P99/P95 metrics reflect real end-to-end latency

**Advantages:**
1. ‚úÖ **Backward compatible** - Requires explicit opt-in
2. ‚úÖ **Flexible** - Supports different completion markers (`[DONE]`, EOF, timeout)
3. ‚úÖ **Useful** - Covers a real use case (LLM streaming)
4. ‚úÖ **Accurate** - Measures what the user actually experiences

---

## üìä Comparison: Before vs After Feature

### Current Scenario (Without Feature)

```
Test with 100 concurrent users:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gatling Report                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ P99 Response Time: 558ms           ‚îÇ ‚Üê Does NOT represent real UX
‚îÇ Mean: 280ms                        ‚îÇ
‚îÇ Errors: 0%                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Reality:
‚Ä¢ Users wait 2-5 seconds for complete responses
‚Ä¢ Team optimizes based on incorrect metrics
‚Ä¢ SLAs are misaligned with actual experience
```

### Proposed Scenario (With Feature)

```
Test with 100 concurrent users:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gatling Report                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Connection P99: 558ms              ‚îÇ ‚Üê Useful for capacity
‚îÇ End-to-End P99: 2,018ms           ‚îÇ ‚Üê NEW - Real UX
‚îÇ TTFT P99: 6ms                     ‚îÇ ‚Üê NEW - Responsiveness
‚îÇ Streaming P99: 2,012ms            ‚îÇ ‚Üê NEW - Processing
‚îÇ Errors: 0%                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Advantages:
‚Ä¢ Metrics aligned with user experience
‚Ä¢ Optimization based on real data
‚Ä¢ Accurate SLAs
```

---

## ü§î Is the Official Documentation Confusing?

### Documentation Quote

https://docs.gatling.io/guides/use-cases/llm-api/

> "Gatling waits for complete stream completion before considering the request finished, not just connection establishment."

### Confusion Analysis

**‚úÖ TECHNICALLY CORRECT:**
- Gatling DOES wait for the stream to complete
- The scenario does NOT continue until `.asLongAs()` finishes
- Gatling does NOT proceed to the next step prematurely

**‚ùå PRACTICALLY MISLEADING:**
- "Waits for" ‚â† "Measures"
- The `.asLongAs()` is a **loop**, not a "request"
- Stream time does **NOT appear** in Gatling metrics
- Users assume "wait" implies "measure"

### Perfect Analogy

```
Gatling says: "I wait for you to finish eating"
User assumes: "Then you measure how long I take to eat"
Reality: Gatling only measures "how long it takes you to sit at the table"
```

### Evidence in Our Reports

```
---- Requests --------------------------------------------------------
> Connect to LLM - short    | P99: 558ms
> close                     | P99: 1ms
```

**Critical observation:** The `.asLongAs()` does **NOT appear** as a request in the report.

---

## üî¨ Deep Analysis of Official Gatling Code

### Official Example Code

https://docs.gatling.io/guides/use-cases/llm-api/

```java
ScenarioBuilder prompt = scenario("Scenario").exec(
  sse("Connect to LLM and get Answer")    // ‚Üê STEP 1
    .post("/completions")
    .header("Authorization", "Bearer " + apiKey)
    .body(StringBody("{\"model\": \"gpt-3.5-turbo\",\"stream\":true,\"messages\":[{\"role\":\"user\",\"content\":\"Just say HI\"}]}"))
    .asJson(),
  asLongAs("#{stop.isUndefined()}").on(   // ‚Üê STEP 2
    sse.processUnmatchedMessages((messages, session) ->
      messages.stream()
        .anyMatch(message -> message.message().contains("{\"data\":\"[DONE]\"}"))
        ? session.set("stop", true) : session;
    )
  ),
  sse("close").close()                     // ‚Üê STEP 3
);
```

### Breakdown: What Does Gatling Measure in Each Step?

#### STEP 1: `sse("Connect to LLM and get Answer").post("/completions")`

```
‚è±Ô∏è  TIMER STARTS
    ‚Üì
üì§ Sends POST request to /completions
    ‚Üì
üîÑ Waits for server response
    ‚Üì
‚úÖ Receives HTTP/1.1 200 OK
    ‚Üì
‚è±Ô∏è  TIMER STOPS ‚Üí Metric captured: "Connect to LLM and get Answer" ‚âà 558ms
```

**What Gatling measures here:**
- ‚úÖ Network latency
- ‚úÖ SSL/TLS handshake time
- ‚úÖ Server initial processing time
- ‚úÖ SSE connection establishment

**What Gatling does NOT measure:**
- ‚ùå Chunk processing
- ‚ùå Streaming time
- ‚ùå User-perceived latency

---

#### STEP 2: `asLongAs("#{stop.isUndefined()}").on(...)`

```
[NO ACTIVE TIMER]
    ‚Üì
üîÅ Loop: Processes incoming SSE messages
    ‚Üì
üì• Receives chunks 1, 2, 3... 100
    ‚Üì
üîç Searches for [DONE] marker in each message
    ‚Üì
‚úÖ Detects [DONE], sets session("stop", true)
    ‚Üì
üîö Exits loop
    ‚Üì
[NO METRIC CAPTURED]
```

**‚ö†Ô∏è CRITICAL: This is a LOOP, NOT a "request"**

- ‚ùå Gatling does NOT have an active timer here
- ‚ùå This part does NOT appear in metrics
- ‚ùå Elapsed time (~1,460ms in our tests) is LOST

**Evidence in Gatling report:**

```
---- Requests --------------------------------------------------------
> Connect to LLM and get Answer    | P99: 558ms     ‚Üê This appears
> close                            | P99: 1ms       ‚Üê This appears
                                                    ‚Üê asLongAs() does NOT appear
```

---

#### STEP 3: `sse("close").close()`

```
‚è±Ô∏è  NEW TIMER STARTS (independent from STEP 1)
    ‚Üì
üîå Closes SSE connection
    ‚Üì
‚è±Ô∏è  TIMER STOPS ‚Üí Metric captured: "close" ‚âà 1ms
```

**What Gatling measures here:**
- ‚úÖ Connection close time (negligible)

---

### Complete Timeline: What Really Happens

```
Time      ‚îÇ Event                               ‚îÇ Gatling Measures ‚îÇ User Experiences
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
0ms       ‚îÇ POST /completions sent              ‚îÇ ‚è±Ô∏è  Timer ON     ‚îÇ [Waiting...]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
100ms     ‚îÇ [SSL/TLS Handshake]                ‚îÇ ‚è±Ô∏è  Measuring    ‚îÇ [Waiting...]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
558ms     ‚îÇ ‚úÖ HTTP 200 OK received             ‚îÇ ‚è±Ô∏è  Timer OFF    ‚îÇ [Waiting...]
          ‚îÇ SSE connection established         ‚îÇ ‚úÖ 558ms         ‚îÇ
          ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          ‚îÇ [asLongAs loop STARTS]             ‚îÇ ‚ùå NOT MEASURED  ‚îÇ [Waiting...]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
564ms     ‚îÇ First chunk received (TTFT)        ‚îÇ ‚ùå NOT MEASURED  ‚îÇ [Sees first text!]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
600ms     ‚îÇ Chunks 2-10 received               ‚îÇ ‚ùå NOT MEASURED  ‚îÇ [Reading...]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
1,000ms   ‚îÇ Chunks 11-50 received              ‚îÇ ‚ùå NOT MEASURED  ‚îÇ [Reading...]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
1,500ms   ‚îÇ Chunks 51-90 received              ‚îÇ ‚ùå NOT MEASURED  ‚îÇ [Reading...]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
2,018ms   ‚îÇ ‚úÖ [DONE] received, chunk 100       ‚îÇ ‚ùå NOT MEASURED  ‚îÇ ‚úÖ [Complete response!]
          ‚îÇ [asLongAs loop ENDS]               ‚îÇ                  ‚îÇ
          ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
2,019ms   ‚îÇ close() invoked                    ‚îÇ ‚è±Ô∏è  Timer ON     ‚îÇ [Already has response]
          ‚îÇ                                     ‚îÇ                  ‚îÇ
2,020ms   ‚îÇ Connection closed                  ‚îÇ ‚è±Ô∏è  Timer OFF    ‚îÇ [Already has response]
          ‚îÇ                                     ‚îÇ ‚úÖ 1ms           ‚îÇ
```

**Metrics summary:**
- **Gatling reports**: 558ms (connection) + 1ms (close) = 559ms total
- **User experiences**: 2,018ms (from request to complete response)
- **Gap**: 1,459ms (261% difference)

---

### üéØ Why is `asLongAs()` NOT Measured?

**Architectural reason in Gatling:**

In Gatling, a "request" is an atomic unit that has:
1. A name (string identifier)
2. An HTTP method (POST, GET, etc.)
3. A timer that starts and stops automatically

**`asLongAs()` is NOT a request, it's a control flow:**

```java
// This IS a request (has name and HTTP method)
sse("Connect to LLM and get Answer").post("/completions")

// This is NOT a request (it's a loop)
asLongAs("#{stop.isUndefined()}").on(...)

// This IS a request (has name and SSE action)
sse("close").close()
```

**Analogy with imperative code:**

```java
long startTime = System.currentTimeMillis();
HttpResponse response = httpClient.post("/completions");  // ‚Üê Gatling measures this
long gatlingMetric = System.currentTimeMillis() - startTime;

// The loop is NOT measured
while (!done) {                                           // ‚Üê Gatling does NOT measure this
    String chunk = readNextChunk();
    if (chunk.contains("[DONE]")) done = true;
}

connection.close();  // ‚Üê Gatling measures this (as separate request)
```

---

### üìä Comparison: Official Code vs Our Code (Sprint 1)

| Aspect | Official Gatling Code | Our Code (Sprint 1) |
|--------|----------------------|---------------------|
| **Structure** | 3 separate steps | 3 steps + manual timing |
| **Connection measurement** | ‚úÖ `sse("Connect...").post()` = 558ms | ‚úÖ Automatic by Gatling |
| **Streaming measurement** | ‚ùå NO - just loop without timer | ‚úÖ `requestStartTime` ‚Üí `currentTime` |
| **TTFT** | ‚ùå NOT captured | ‚úÖ First `delta.content` timestamp |
| **Total Response Time** | ‚ùå Only 558ms (connection) | ‚úÖ 2,018ms (real end-to-end) |
| **Metrics in report** | ‚úÖ P99 of connection (incomplete) | ‚úÖ P99 of complete experience |
| **Truncation Detection** | ‚ùå Only loop timeout | ‚úÖ Timeout + buffer overflow |
| **Test Phase Tracking** | ‚ùå NO | ‚úÖ RAMP vs STEADY |
| **Export Format** | ‚úÖ Gatling HTML report | ‚úÖ JSONL + Gatling report |
| **Post-analysis** | ‚ùå Limited to Gatling metrics | ‚úÖ 5-dimensional quality |

**Sprint 1 code (manual timing):**

```java
// Timer initialization (INSIDE asLongAs)
long requestStartTime = session.contains("requestStartTime")
    ? session.getLong("requestStartTime")
    : System.currentTimeMillis();

if (!session.contains("requestStartTime")) {
    session = session.set("requestStartTime", requestStartTime);
}

// ... chunk processing ...

// When detecting [DONE] or timeout
if (done || timedOut) {
    long currentTime = System.currentTimeMillis();
    long responseTimeMs = currentTime - requestStartTime;  // ‚Üê THIS is the real metric

    // responseTimeMs = 2,018ms (vs Gatling = 558ms)
}
```

---

### üí° Key Insights from Code Analysis

#### 1. **The request name is misleading**

```java
sse("Connect to LLM and get Answer")  // ‚Üê Says "get Answer"
  .post("/completions")               // ‚Üê Only measures "Connect", NOT "get Answer"
```

**The name suggests:** Measure until getting the answer
**The reality:** Only measures until establishing connection

**This reinforces the feature request argument:**
- Users expect a request named "Connect **and get Answer**" to measure both
- Natural expectation is to include the complete response
- Current behavior is counter-intuitive

---

#### 2. **Artificial separation between connection and streaming**

From the user's perspective, this is **ONE operation**:

```
User asks question ‚Üí Waits for complete response
```

But Gatling divides it into:

```
1. Connection (measured)
2. Streaming (NOT measured)  ‚Üê Artificial from UX perspective
3. Close (measured)
```

---

#### 3. **Official code DEMONSTRATES the need for the feature**

The fact that Gatling provides this code as an official example but:
- ‚ùå Does NOT capture TTFT
- ‚ùå Does NOT capture complete response time
- ‚ùå Does NOT capture streaming metrics

...demonstrates that the framework **needs to evolve** for this use case.

---

### üéØ Improved API Proposal

Based on the official code, we propose:

#### Option 1: Unify in a single request (simpler)

```java
sse("Connect to LLM and get Answer")
  .post("/completions")
  .body(StringBody("{...}"))
  .measureUntilStreamCompletion()        // ‚Üê NEW: Extends timer
  .completionMarker("[DONE]")            // ‚Üê NEW: Defines marker
  .timeout(10, TimeUnit.SECONDS)         // ‚Üê NEW: Explicit timeout
  .asJson()
// asLongAs() no longer needed - Gatling handles it internally
```

**Behavior:**
- Timer does NOT stop at HTTP 200 OK
- Gatling processes chunks internally
- Timer stops when detecting `[DONE]` or timeout
- P99 metrics reflect complete experience

---

#### Option 2: Separate request for streaming (more flexible)

```java
scenario("Scenario").exec(
  sse("Connect to LLM")
    .post("/completions")
    .asJson(),

  sse("Process Stream")                  // ‚Üê NEW: Request type for streaming
    .measureStreamDuration()
    .asLongAs("#{stop.isUndefined()}").on(
      sse.processUnmatchedMessages(...)
    ),

  sse("close").close()
)
```

**Advantage:** Separate reports for connection vs streaming

```
---- Requests --------------------------------------------------------
> Connect to LLM        | P99: 558ms    ‚Üê Connection capacity
> Process Stream        | P99: 1,460ms  ‚Üê Streaming latency  ‚Üê NEW
> close                 | P99: 1ms      ‚Üê Close
```

---

### üîç Design Questions for Feature Request

#### 1. **What happens if `[DONE]` never arrives?**

```java
.measureUntilStreamCompletion()
.completionMarker("[DONE]")
.timeout(10, TimeUnit.SECONDS)         // ‚Üê Required
.onTimeout(MarkAs.ERROR)               // ‚Üê Or MarkAs.SUCCESS with flag
```

**Options:**
- `MarkAs.ERROR` ‚Üí Request fails, appears in "Errors" in report
- `MarkAs.SUCCESS` ‚Üí Request completes successfully, but flag indicates timeout

---

#### 2. **Support multiple completion markers?**

Different LLM APIs use different markers:

```java
.completionMarkers(Arrays.asList(
    "[DONE]",                    // OpenAI
    "data: [DONE]",              // OpenAI variant
    "{\"finish_reason\":\"stop\"}"  // Anthropic/others
))
.orStreamEnd()                   // Or detect stream EOF
```

---

#### 3. **How to capture TTFT in addition to response time?**

```java
sse("Connect to LLM")
  .post("/completions")
  .measureUntilStreamCompletion()
  .captureTimeToFirstData()      // ‚Üê NEW: Captures TTFT
  .completionMarker("[DONE]")
```

**Resulting report:**

```
---- Requests --------------------------------------------------------
> Connect to LLM        | TTFT P99: 6ms | Total P99: 2,018ms
```

---

## üéØ Recommendation

### 1. Yes, It's a Valid Feature Request

**Reasons:**
- ‚úÖ Covers a real and growing use case (LLM streaming)
- ‚úÖ Current implementation leads to misinterpretation
- ‚úÖ Other users probably have the same problem
- ‚úÖ Gatling positions itself as a tool for LLM testing

### 2. No, Gatling does NOT "confuse" the protocol

**Clarification:**
- ‚ùå Gatling does NOT misinterpret SSE
- ‚ùå Gatling does NOT violate HTTP standards
- ‚úÖ Gatling simply was not designed for this use case
- ‚úÖ Current behavior is correct from HTTP perspective

### 3. The Gap is in Expectation vs Reality

**The real problem:**
```
Expectation: "Measure user-perceived latency"
Reality: "Measure HTTP connection establishment latency"
Gap: These are different in streaming, same in traditional HTTP
```

---

## üìù GitHub Issue Proposal

### Suggested Title
```
Feature Request: Add optional end-to-end latency measurement for SSE streaming
```

### Suggested Content

```markdown
## Context

When load testing LLM APIs with Server-Sent Events (SSE), Gatling correctly
measures HTTP connection establishment (~500ms) but not the full streaming
duration (~2000ms). For user experience metrics, we need end-to-end latency.

## Current Behavior

Using the official example from https://docs.gatling.io/guides/use-cases/llm-api/:

```java
ScenarioBuilder prompt = scenario("Scenario").exec(
  sse("Connect to LLM and get Answer")    // ‚Üê Timer starts/stops here
    .post("/completions")
    .body(StringBody("{...}"))
    .asJson(),
  asLongAs("#{stop.isUndefined()}").on(   // ‚Üê NO timer here
    sse.processUnmatchedMessages((messages, session) ->
      messages.stream()
        .anyMatch(message -> message.message().contains("[DONE]"))
        ? session.set("stop", true) : session
    )
  ),
  sse("close").close()
);
```

**Problem:** The `asLongAs()` loop (where streaming happens) is NOT measured.

**Gatling report shows:**
```
---- Requests --------------------------------------------------------
> Connect to LLM and get Answer    | P99: 558ms
> close                            | P99: 1ms
                                    ‚Üê asLongAs loop missing
```

**What Gatling measures:** Connection setup (558ms in our tests)
**What user experiences:** Full response time (2,018ms in our tests)
**Gap:** 261% difference (1,460ms of streaming NOT captured)

## Proposed Feature

### Option 1: Extend timer until stream completion (simpler)

```java
sse("Connect to LLM and get Answer")
  .post("/completions")
  .body(StringBody("{...}"))
  .measureUntilStreamCompletion()        // ‚Üê NEW: Timer doesn't stop at HTTP 200
  .completionMarker("[DONE]")            // ‚Üê NEW: Define completion condition
  .timeout(10, TimeUnit.SECONDS)         // ‚Üê NEW: Explicit timeout
  .asJson()
// The asLongAs() could be handled internally by Gatling
```

**Result:** Single metric that represents full user experience (connection + streaming).

---

### Option 2: Separate measurable request for streaming (more flexible)

```java
scenario("Scenario").exec(
  sse("Connect to LLM")
    .post("/completions")
    .asJson(),

  sse("Process Stream")                  // ‚Üê NEW: Measurable streaming request
    .measureStreamDuration()
    .completionMarker("[DONE]")
    .asLongAs("#{stop.isUndefined()}").on(
      sse.processUnmatchedMessages(...)
    ),

  sse("close").close()
)
```

**Result:** Separate metrics for connection vs streaming (better for analysis).

```
---- Requests --------------------------------------------------------
> Connect to LLM        | P99: 558ms    ‚Üê Connection capacity
> Process Stream        | P99: 1,460ms  ‚Üê Streaming latency (NEW)
> close                 | P99: 1ms      ‚Üê Close
```

Both options would include streaming time in Gatling's P99/P95 metrics.

## Benefits

1. Accurate UX metrics for streaming APIs
2. Proper SLA definition for LLM services
3. Aligned with growing LLM testing use case
4. Backward compatible (opt-in)

## Workaround (Current)

We implemented manual timing in session:
- Capture `requestStartTime` before request
- Calculate `responseTimeMs` after `[DONE]`
- Export to custom JSONL for analysis

This works but loses Gatling's built-in percentile calculations.

## Evidence

- Official guide: https://docs.gatling.io/guides/use-cases/llm-api/
- Our analysis: [link to TTFT_PERCENTIL99_ANALYSIS.md]

## Why This Matters

The official example names the request **"Connect to LLM and get Answer"** but only measures the "Connect" part, not the "get Answer" part. This creates a gap between:

- **User expectation:** "I want to measure how long it takes to get an answer"
- **Gatling behavior:** "I measure how long it takes to establish the connection"

For LLM applications, the answer **IS** the streaming phase, not just the connection.

## Additional Considerations

### 1. TTFT (Time To First Token)
Consider also capturing time to first data chunk:

```java
.captureTimeToFirstData()  // Captures TTFT separately
```

**Report output:**
```
> Connect to LLM    | TTFT P99: 6ms | Total P99: 2,018ms
```

### 2. Multiple completion markers
Different LLM APIs use different markers:

```java
.completionMarkers(Arrays.asList("[DONE]", "data: [DONE]"))
.orStreamEnd()  // Or detect natural EOF
```

### 3. Timeout handling
Clear semantics for when completion marker never arrives:

```java
.onTimeout(MarkAs.ERROR)     // Fail the request
// OR
.onTimeout(MarkAs.SUCCESS)   // Complete with flag
```
```

---

## ‚úÖ Final Conclusions

### 1. Feature Request: Yes, it's valid

- ‚úÖ Would benefit the LLM testing community
- ‚úÖ Aligned with Gatling's direction (LLM use cases)
- ‚úÖ Technically feasible solution
- ‚úÖ Backward compatible

### 2. Gatling is not "wrong"

- ‚úÖ Correct behavior according to HTTP/SSE standards
- ‚úÖ Designed for traditional request/response
- ‚úÖ Not a bug, it's a feature gap

### 3. Documentation could improve

**Improvement suggestion:**

```markdown
## ‚ö†Ô∏è Important Note on SSE Metrics

Gatling waits for stream completion before proceeding to the next step,
but **does not include streaming time in request metrics**.

The `.asLongAs()` loop processes events but is not measured as part of
the request's response time. Only the initial HTTP connection setup is
included in P99/P95 statistics.

For end-to-end latency measurement (including full stream processing),
use custom session timing or see [Feature Request #XXXX].
```

### 4. Sprint 1 remains the best current solution

- ‚úÖ Manual measurement is NECESSARY today
- ‚úÖ Our approach is correct and complete
- ‚úÖ Captures metrics that Gatling cannot capture natively

---

## üéØ Recommended Next Steps

1. **Continue using Sprint 1** - It's the only way to get accurate metrics today
2. **Optional:** Open GitHub Issue in Gatling for future feature
3. **Document internally** - Explain why we use manual measurement
4. **Evangelize** - Share findings with the community

---

## üìã Executive Summary for Gatling Team

### TL;DR

The official example code for LLM testing (https://docs.gatling.io/guides/use-cases/llm-api/) has a critical gap:

**The request is named** "Connect to LLM **and get Answer**"
**But only measures** "Connect to LLM" (not "get Answer")

72% of response time (the streaming part) is not captured in metrics.

---

### Impact

- **261% underestimation** in reported latency (558ms vs 2,018ms actual)
- **Incorrect SLAs** based on incomplete metrics
- **Misdirected optimizations** toward components that are not the bottleneck
- **Gap between metrics and user experience** in LLM applications

---

### Root Cause

The `asLongAs()` loop (where streaming occurs) is a **control flow**, not a **measurable request**:

```java
sse("Connect to LLM and get Answer").post("/completions")  // ‚Üê Measured: 558ms
asLongAs("#{stop.isUndefined()}").on(...)                  // ‚Üê NOT measured: 1,460ms
sse("close").close()                                        // ‚Üê Measured: 1ms
```

**Total measured:** 559ms | **Total actual:** 2,018ms | **Gap:** 1,459ms (72%)

---

### Proposed Solution

Option 1 (simple):
```java
.measureUntilStreamCompletion().completionMarker("[DONE]")
```

Option 2 (flexible):
```java
sse("Process Stream").measureStreamDuration().asLongAs(...)
```

**Benefits:** Backward compatible, opt-in, solves growing LLM use case

---

### Validation

- ‚úÖ We implemented manual measurement (Sprint 1) - works perfectly
- ‚úÖ Confirmed against RFC 6202 (SSE) and official documentation
- ‚úÖ Gatling's current behavior is correct per HTTP standard
- ‚úÖ Feature request aligned with Gatling's direction toward LLM testing

---

### Community Impact

With the explosion of LLM applications, many teams likely face this same problem. This feature would benefit:

- ‚úÖ QA teams testing LLM APIs
- ‚úÖ Engineers defining SLAs for streaming services
- ‚úÖ Product teams optimizing UX of conversational applications
- ‚úÖ Performance engineers measuring perceived latency

---

**Last updated**: October 22, 2025
**Author**: Post-Sprint 1 Analysis (Load Testing LLM SSE)
**Contact**: [Your contact information for GitHub]
**References**:
- TTFT_PERCENTIL99_ANALYSIS.md (detailed analysis of our tests)
- https://docs.gatling.io/guides/use-cases/llm-api/ (official documentation)
- RFC 6202 (Server-Sent Events)
- This document: GATLING_FEATURE_REQUEST_ANALYSIS.md
