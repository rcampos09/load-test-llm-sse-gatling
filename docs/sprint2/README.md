# üß† Sprint 2: An√°lisis Avanzado con LLM

**Estado**: üìù Planificado
**Duraci√≥n estimada**: 1 semana (5 d√≠as h√°biles)
**Dependencias**: Sprint 1 completado ‚úÖ

---

## üéØ Objetivos

Sprint 2 se enfoca en **an√°lisis sem√°ntico avanzado** usando LLM-as-a-judge para evaluar la calidad del contenido generado.

**Principales entregables:**
1. **Integraci√≥n con LLM** para an√°lisis sem√°ntico (GPT-4)
2. **Prompt engineering** optimizado para evaluaci√≥n de respuestas
3. **QualityReportGenerator.java** con m√©tricas agregadas
4. **M√©tricas avanzadas** de coherencia, relevancia y correcci√≥n t√©cnica

---

## üìã Tareas Sprint 2

### ‚úÖ **Tarea 2.2: Integraci√≥n con LLM para An√°lisis Sem√°ntico**

**Objetivo**: Reemplazar Jaccard similarity con evaluaci√≥n LLM real.

**Implementaci√≥n:**

```java
public class LLMAnalyzer {
    private final OpenAIClient client;

    public SemanticAnalysisResult analyzeSimilarity(
        String prompt,
        List<String> responses
    ) {
        // Usar GPT-4 para comparar respuestas del mismo prompt
        String evaluationPrompt = buildEvaluationPrompt(prompt, responses);

        ChatCompletion completion = client.chat()
            .model("gpt-4")
            .messages(List.of(
                new Message("system", EVALUATION_SYSTEM_PROMPT),
                new Message("user", evaluationPrompt)
            ))
            .create();

        return parseSemanticResult(completion.choices().get(0).message().content());
    }
}
```

**Outputs esperados:**
- Similarity score (0-1) basado en similitud sem√°ntica real
- Identificaci√≥n de variaciones leg√≠timas (creatividad) vs inconsistencias t√©cnicas
- Detecci√≥n de alucinaciones o informaci√≥n incorrecta

**Estimado**: 2 d√≠as

---

### ‚úÖ **Tarea 2.3: Implementar Prompt Mejorado**

**Objetivo**: Dise√±ar prompts efectivos para que GPT-4 eval√∫e calidad de respuestas.

**Prompt de evaluaci√≥n (draft):**

```markdown
# System Prompt
Eres un evaluador experto de respuestas de LLMs. Tu trabajo es analizar respuestas
generadas por GPT-3.5-turbo y evaluar su calidad seg√∫n criterios objetivos.

# Evaluation Prompt Template
Analiza las siguientes respuestas al prompt: "{original_prompt}"

**Respuestas a evaluar:**
1. {response_1}
2. {response_2}
...
N. {response_N}

**Criterios de evaluaci√≥n:**

1. **Similitud Sem√°ntica (0-10)**: ¬øLas respuestas comunican el mismo mensaje?
   - 10: Id√©nticas en significado
   - 7-9: Variaciones de estilo pero mismo contenido
   - 4-6: Algunas diferencias de contenido
   - 0-3: Contenido significativamente diferente

2. **Correcci√≥n T√©cnica (0-10)**: Para prompts t√©cnicos, ¬ølas respuestas son correctas?
   - Validar precisi√≥n de c√≥digo, conceptos, terminolog√≠a

3. **Coherencia (0-10)**: ¬øLas respuestas son l√≥gicas y completas?

4. **Detecci√≥n de Problemas**:
   - ¬øHay alucinaciones (informaci√≥n inventada)?
   - ¬øHay respuestas que contradigan el prompt?
   - ¬øHay respuestas incompletas o cortadas?

**Output esperado (JSON):**
{
  "similarity_score": 0.85,
  "technical_correctness": 9.2,
  "coherence_score": 8.5,
  "issues_detected": [
    "Response 3 contains hallucinated information about X",
    "Response 7 is incomplete (truncated)"
  ],
  "legitimate_variations": [
    "Responses use different code implementations (recursion vs iteration) but both correct"
  ]
}
```

**Validaci√≥n del prompt:**
- Testear con 5-10 casos conocidos (prompts creativos vs t√©cnicos)
- Validar que distingue creatividad de inconsistencia
- Ajustar seg√∫n falsos positivos/negativos

**Estimado**: 1 d√≠a

---

### ‚úÖ **Tarea 3.1: QualityReportGenerator.java**

**Objetivo**: Generar reportes agregados con scores de calidad por dimensi√≥n.

**Implementaci√≥n:**

```java
public class QualityReportGenerator {

    public QualityReport generateReport(List<ResponseMetadata> responses) {
        QualityReport report = new QualityReport();

        // Agrupar por prompt
        Map<String, List<ResponseMetadata>> byPrompt =
            responses.stream()
                .collect(Collectors.groupingBy(ResponseMetadata::getPrompt));

        // Para cada prompt, analizar consistencia
        for (Map.Entry<String, List<ResponseMetadata>> entry : byPrompt.entrySet()) {
            String prompt = entry.getKey();
            List<ResponseMetadata> promptResponses = entry.getValue();

            // An√°lisis b√°sico (Sprint 1)
            BasicAnalysis basic = analyzeBasic(promptResponses);

            // An√°lisis LLM (Sprint 2 - NUEVO)
            LLMAnalysis llm = llmAnalyzer.analyzeSimilarity(
                prompt,
                extractTexts(promptResponses)
            );

            PromptQualityScore score = new PromptQualityScore(
                prompt,
                basic.truncationRate(),
                basic.avgResponseTime(),
                llm.similarityScore(),
                llm.technicalCorrectness(),
                llm.coherenceScore()
            );

            report.addPromptScore(score);
        }

        // Scores globales
        report.setGlobalConsistencyScore(calculateGlobalScore(report));
        report.setTimestamp(Instant.now());

        return report;
    }
}
```

**Output JSON:**

```json
{
  "timestamp": "2025-10-26T10:30:00Z",
  "global_consistency_score": 0.752,
  "total_requests": 610,
  "summary": {
    "truncation_rate": 0.475,
    "avg_similarity": 0.82,
    "avg_technical_correctness": 8.5,
    "avg_coherence": 8.1
  },
  "by_prompt": [
    {
      "prompt": "Implementa b√∫squeda binaria en Java",
      "responses_count": 20,
      "truncation_rate": 0.15,
      "similarity_score": 0.75,
      "technical_correctness": 9.2,
      "coherence": 8.8,
      "issues": ["2 responses truncated"]
    }
  ]
}
```

**Estimado**: 1 d√≠a

---

### ‚úÖ **Tarea 3.2: M√©tricas Avanzadas**

**Objetivo**: Expandir m√©tricas m√°s all√° de lo b√°sico de Sprint 1.

**Nuevas m√©tricas:**

1. **Prompt Category Performance**
   - Score por categor√≠a (short, medium, long, creative, technical)
   - Identificar qu√© categor√≠as tienen peor calidad bajo carga

2. **Degradation by Phase**
   - Comparar RAMP vs STEADY en todas las m√©tricas
   - No solo latencia, tambi√©n calidad sem√°ntica

3. **Quality vs Speed Correlation**
   - ¬øRespuestas m√°s r√°pidas son de menor calidad?
   - Plot: response_time vs semantic_score

4. **False Positive Rate (Sprint 2)**
   - Comparar Jaccard (Sprint 1) vs LLM (Sprint 2)
   - Documentar cu√°ntos falsos positivos eliminamos

**Implementaci√≥n:**

```java
public class AdvancedMetrics {

    public CategoryPerformance analyzeByCategoryPerformance(QualityReport report) {
        // Agrupar por categor√≠a y calcular scores promedio
        return report.getPromptScores().stream()
            .collect(Collectors.groupingBy(
                PromptQualityScore::getCategory,
                Collectors.averagingDouble(PromptQualityScore::getOverallScore)
            ));
    }

    public PhaseComparison comparePhases(QualityReport report) {
        List<PromptQualityScore> rampScores = filterByPhase(report, "RAMP");
        List<PromptQualityScore> steadyScores = filterByPhase(report, "STEADY");

        return new PhaseComparison(
            avgScore(rampScores),
            avgScore(steadyScores),
            calculateDegradation(rampScores, steadyScores)
        );
    }
}
```

**Estimado**: 1 d√≠a

---

## üìä M√©tricas de √âxito Sprint 2

| M√©trica | Sprint 1 (Actual) | Sprint 2 (Objetivo) |
|---------|-------------------|---------------------|
| **An√°lisis Sem√°ntico** | Jaccard 0.306 (no confiable) | LLM score >0.7 (confiable) |
| **Falsos Positivos** | Muchos (no cuantificado) | <10% validado |
| **Cobertura de An√°lisis** | 100% (Jaccard simple) | 20-30% (LLM muestreado por costo) |
| **Detecci√≥n Alucinaciones** | No | S√≠ (via LLM-as-judge) |
| **Costo por Test** | $0.30 (solo generaci√≥n) | $0.30 + $2-3 (an√°lisis LLM) ‚âà $3 total |

---

## üí∞ Presupuesto Sprint 2

| Componente | Costo Estimado | Justificaci√≥n |
|------------|----------------|---------------|
| Test original (610 responses) | $0.30 | Sprint 1 (sin cambios) |
| An√°lisis LLM (20% sampling) | $2-3 | GPT-4 evaluando 120 respuestas |
| Iteraci√≥n de prompts | $0.50 | Testing del evaluation prompt |
| **Total Sprint 2** | **~$3.50** | Por test completo |

**Estrategia de muestreo:**
- Evaluar 20-30% de respuestas con LLM (no 100% por costo)
- Priorizar prompts con baja similitud Jaccard (posibles problemas)
- Validar manualmente subset de resultados LLM

---

## üîÑ Plan de Implementaci√≥n (5 d√≠as)

### **D√≠a 1: Setup y Prompt Engineering**
- [ ] Setup de OpenAI API con GPT-4
- [ ] Dise√±o del evaluation prompt
- [ ] Testing con 5-10 casos conocidos
- [ ] Ajuste de prompt seg√∫n resultados

### **D√≠a 2: Integraci√≥n LLM**
- [ ] Implementar `LLMAnalyzer.java`
- [ ] Integrar con `ConsistencyAnalyzer` de Sprint 1
- [ ] Testing de integraci√≥n
- [ ] Manejo de rate limits y errores

### **D√≠a 3: QualityReportGenerator**
- [ ] Implementar `QualityReportGenerator.java`
- [ ] Formato JSON del reporte
- [ ] Validar con datos de Sprint 1
- [ ] Testing unitario

### **D√≠a 4: M√©tricas Avanzadas**
- [ ] Implementar `AdvancedMetrics.java`
- [ ] Category performance analysis
- [ ] Phase comparison (RAMP vs STEADY)
- [ ] Quality vs Speed correlation

### **D√≠a 5: Testing y Documentaci√≥n**
- [ ] Re-ejecutar test de 610 requests
- [ ] Generar reporte Sprint 2
- [ ] Comparar Sprint 1 vs Sprint 2
- [ ] Documentar lecciones aprendidas
- [ ] Update `SPRINT2_VALIDATION_REPORT.md`

---

## üéØ Entregables Sprint 2

1. **C√≥digo:**
   - `src/test/java/ssellm/LLMAnalyzer.java`
   - `src/test/java/ssellm/QualityReportGenerator.java`
   - `src/test/java/ssellm/AdvancedMetrics.java`

2. **Reportes:**
   - `target/quality_report_sprint2.json`
   - `docs/sprint2/validation-report.md`
   - Comparativa Sprint 1 vs Sprint 2

3. **Documentaci√≥n:**
   - Evaluation prompts usados
   - An√°lisis de falsos positivos eliminados
   - Recomendaciones para Sprint 3

---

## üö® Riesgos y Mitigaciones

### Riesgo 1: Costo de GPT-4
**Impacto**: $3-5 por test completo puede ser prohibitivo
**Mitigaci√≥n**: Sampling (20-30%), no evaluar todo al 100%

### Riesgo 2: Rate Limits de OpenAI
**Impacto**: Test puede fallar si excedemos l√≠mites
**Mitigaci√≥n**: Implement exponential backoff, retry logic

### Riesgo 3: Consistencia de GPT-4
**Impacto**: Evaluaciones pueden variar entre ejecuciones
**Mitigaci√≥n**: Temperature=0, validar con 3 ejecuciones del mismo caso

---

## üìö Referencias

- [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat)
- [LLM-as-a-Judge Best Practices](https://www.deeplearning.ai/short-courses/quality-safety-llm-applications/)
- Sprint 1: `docs/sprint1/consistency-article.md`

---

**Estado**: Planificado | **Owner**: Ricardo Campos | **√öltima actualizaci√≥n**: Octubre 2025
