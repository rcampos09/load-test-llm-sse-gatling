# üìä Sprint 3: Visualizaci√≥n y Automatizaci√≥n

**Estado**: üìù Planificado
**Duraci√≥n estimada**: 3-4 d√≠as h√°biles
**Dependencias**: Sprint 2 completado ‚úÖ

---

## üéØ Objetivos

Sprint 3 se enfoca en **visualizaci√≥n** y **automatizaci√≥n** del proceso de an√°lisis de calidad.

**Principales entregables:**
1. **Dashboard HTML interactivo** con gr√°ficos de calidad
2. **Script automatizado** para ejecutar test + an√°lisis + reporte
3. **Sistema de umbrales configurables** para alertas
4. **Documentaci√≥n actualizada** con nuevos flujos

---

## üìã Tareas Sprint 3

### ‚úÖ **Tarea 3.3: Dashboard HTML Interactivo**

**Objetivo**: Generar dashboard visual para explorar resultados del an√°lisis de calidad.

**Implementaci√≥n:**

```java
public class DashboardGenerator {

    public void generateDashboard(QualityReport report) {
        String htmlContent = buildDashboard(report);
        Files.writeString(
            Paths.get("target/quality_dashboard.html"),
            htmlContent
        );
    }

    private String buildDashboard(QualityReport report) {
        return """
        <!DOCTYPE html>
        <html>
        <head>
            <title>LLM Quality Analysis Dashboard</title>
            <script src="https://cdn.plot.ly/plotly-2.26.0.min.js"></script>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .metric-card {
                    border: 1px solid #ddd;
                    padding: 15px;
                    margin: 10px;
                    border-radius: 8px;
                }
                .critical { border-left: 5px solid #e74c3c; }
                .warning { border-left: 5px solid #f39c12; }
                .success { border-left: 5px solid #27ae60; }
            </style>
        </head>
        <body>
            <h1>üß† LLM Load Test Quality Report</h1>

            <!-- Global Metrics -->
            <div class="metrics-summary">
                """ + buildGlobalMetrics(report) + """
            </div>

            <!-- Charts -->
            <div id="truncationByCategory"></div>
            <div id="latencyByPhase"></div>
            <div id="qualityByPrompt"></div>
            <div id="timeSeriesQuality"></div>

            <script>
                """ + buildChartScripts(report) + """
            </script>
        </body>
        </html>
        """;
    }
}
```

**Gr√°ficos incluidos:**

1. **Truncation Rate by Category** (bar chart)
   - Comparar short (8.3%) vs medium (53.3%) vs long (70.4%)

2. **Latency Distribution by Phase** (box plot)
   - RAMP (1,009ms) vs STEADY (8,826ms)

3. **Quality Score by Prompt** (horizontal bar chart)
   - Ordenar por similarity_score + technical_correctness

4. **Quality Over Time** (line chart)
   - Mostrar degradaci√≥n temporal durante el test

**Output esperado:**
- `target/quality_dashboard.html` (archivo standalone)
- Interactivo (zoom, hover tooltips)
- Responsive (mobile-friendly)

**Estimado**: 1.5 d√≠as

---

### ‚úÖ **Tarea 4.1: Script Automatizado de Test Completo**

**Objetivo**: Ejecutar todo el flujo con un solo comando.

**Implementaci√≥n:**

```bash
#!/bin/bash
# run_quality_test.sh

set -e  # Exit on error

echo "üöÄ Starting LLM Load Test with Quality Analysis..."

# Step 1: Clean previous results
echo "üìÅ Cleaning previous results..."
rm -rf target/gatling target/*.json target/*.jsonl target/*.html

# Step 2: Run Gatling load test
echo "‚ö° Running Gatling load test..."
./mvnw gatling:test -Dgatling.simulationClass=ssellm.LoadTestSimulation

# Step 3: Parse responses
echo "üìä Parsing responses..."
java -cp target/test-classes ssellm.ResponseParser

# Step 4: Validate responses
echo "‚úÖ Validating response quality..."
java -cp target/test-classes ssellm.ResponseValidator

# Step 5: Run consistency analysis (Sprint 1)
echo "üîç Running consistency analysis..."
java -cp target/test-classes ssellm.ConsistencyAnalyzer

# Step 6: Run LLM analysis (Sprint 2 - sampling 20%)
echo "üß† Running LLM semantic analysis (20% sampling)..."
java -cp target/test-classes ssellm.LLMAnalyzer --sample-rate=0.2

# Step 7: Generate quality report
echo "üìà Generating quality report..."
java -cp target/test-classes ssellm.QualityReportGenerator

# Step 8: Generate dashboard
echo "üé® Generating HTML dashboard..."
java -cp target/test-classes ssellm.DashboardGenerator

# Step 9: Check thresholds
echo "üö® Checking quality thresholds..."
java -cp target/test-classes ssellm.ThresholdChecker

echo ""
echo "‚úÖ Test completed successfully!"
echo "üìä Dashboard: target/quality_dashboard.html"
echo "üìÑ Report: target/quality_report.json"
echo ""
```

**Caracter√≠sticas:**
- Exit on error (detiene si algo falla)
- Logging claro de cada paso
- Tiempo estimado de ejecuci√≥n: ~3 minutos (sin LLM) / ~8 minutos (con LLM sampling)

**Estimado**: 0.5 d√≠as

---

### ‚úÖ **Tarea 4.2: Sistema de Umbrales Configurables**

**Objetivo**: Definir SLAs y generar alertas autom√°ticas si se violan.

**Implementaci√≥n:**

```java
public class ThresholdChecker {

    public static class Thresholds {
        // Availability
        double minAvailability = 0.95;  // 95% requests must succeed

        // Quality
        double minGlobalConsistency = 0.70;  // 70% overall quality
        double maxTruncationRate = 0.20;     // Max 20% truncated

        // Semantic (LLM)
        double minSemanticSimilarity = 0.75;  // 75% similarity
        double minTechnicalCorrectness = 8.0; // 8/10 score

        // Latency
        double maxP95Latency = 10000;  // 10s P95
        double maxP99Latency = 15000;  // 15s P99

        // By category
        Map<String, Double> maxTruncationByCategory = Map.of(
            "short", 0.10,   // Max 10% for short prompts
            "medium", 0.30,  // Max 30% for medium
            "long", 0.50     // Max 50% for long
        );
    }

    public ThresholdReport check(QualityReport report, Thresholds thresholds) {
        List<Violation> violations = new ArrayList<>();

        // Check availability
        double availability = (double) report.getSuccessfulRequests() / report.getTotalRequests();
        if (availability < thresholds.minAvailability) {
            violations.add(new Violation(
                "CRITICAL",
                "Availability",
                String.format("%.2f%% < %.2f%%", availability * 100, thresholds.minAvailability * 100)
            ));
        }

        // Check truncation rate
        if (report.getTruncationRate() > thresholds.maxTruncationRate) {
            violations.add(new Violation(
                "CRITICAL",
                "Truncation Rate",
                String.format("%.2f%% > %.2f%%", report.getTruncationRate() * 100, thresholds.maxTruncationRate * 100)
            ));
        }

        // Check semantic similarity (LLM)
        if (report.getAvgSimilarity() < thresholds.minSemanticSimilarity) {
            violations.add(new Violation(
                "WARNING",
                "Semantic Similarity",
                String.format("%.2f < %.2f", report.getAvgSimilarity(), thresholds.minSemanticSimilarity)
            ));
        }

        // Check by category
        for (Map.Entry<String, CategoryStats> entry : report.getCategoryStats().entrySet()) {
            String category = entry.getKey();
            double truncationRate = entry.getValue().getTruncationRate();
            double threshold = thresholds.maxTruncationByCategory.get(category);

            if (truncationRate > threshold) {
                violations.add(new Violation(
                    "WARNING",
                    "Category: " + category,
                    String.format("Truncation %.2f%% > %.2f%%", truncationRate * 100, threshold * 100)
                ));
            }
        }

        return new ThresholdReport(violations);
    }
}
```

**Configuraci√≥n (thresholds.yaml):**

```yaml
thresholds:
  availability:
    min: 0.95  # 95% success rate

  quality:
    global_consistency_min: 0.70  # 70%
    truncation_rate_max: 0.20     # 20%

  semantic:
    similarity_min: 0.75           # 75%
    technical_correctness_min: 8.0 # 8/10

  latency:
    p95_max_ms: 10000  # 10s
    p99_max_ms: 15000  # 15s

  by_category:
    short:
      truncation_max: 0.10  # 10%
    medium:
      truncation_max: 0.30  # 30%
    long:
      truncation_max: 0.50  # 50%
```

**Output de ejemplo:**

```json
{
  "timestamp": "2025-10-26T10:30:00Z",
  "status": "FAILED",
  "violations": [
    {
      "severity": "CRITICAL",
      "metric": "Truncation Rate",
      "message": "47.50% > 20.00%",
      "actual_value": 0.475,
      "threshold": 0.20
    },
    {
      "severity": "WARNING",
      "metric": "Category: long",
      "message": "Truncation 70.37% > 50.00%",
      "actual_value": 0.7037,
      "threshold": 0.50
    }
  ],
  "exit_code": 1
}
```

**Integraci√≥n con CI/CD:**
```bash
# En GitHub Actions / Jenkins
./run_quality_test.sh
EXIT_CODE=$?

if [ $EXIT_CODE -ne 0 ]; then
    echo "‚ùå Quality thresholds violated!"
    # Send Slack notification
    # Fail the build
    exit 1
fi
```

**Estimado**: 1 d√≠a

---

### ‚úÖ **Tarea Sprint 3: Actualizar Documentaci√≥n**

**Objetivo**: Documentar nuevos flujos automatizados.

**Documentos a actualizar:**

1. **README.md principal**
   - Agregar secci√≥n "Quick Start" con `./run_quality_test.sh`
   - Actualizar arquitectura del sistema

2. **docs/sprint3/validation-report.md**
   - Documentar validaci√≥n del dashboard
   - Screenshots de gr√°ficos
   - An√°lisis de usabilidad

3. **docs/sprint3/thresholds-guide.md**
   - Gu√≠a de configuraci√≥n de umbrales
   - Ejemplos de diferentes escenarios (dev, staging, prod)
   - Recomendaciones de SLAs por tipo de sistema

**Estimado**: 1 d√≠a

---

## üìä M√©tricas de √âxito Sprint 3

| M√©trica | Sprint 2 (Manual) | Sprint 3 (Objetivo) |
|---------|-------------------|---------------------|
| **Tiempo de an√°lisis completo** | ~15 min manual | <5 min automatizado |
| **Pasos manuales requeridos** | 7-8 comandos | 1 script |
| **Interpretaci√≥n de resultados** | Leer JSON raw | Dashboard visual |
| **Detecci√≥n de violaciones** | Manual | Autom√°tica + alertas |
| **Configurabilidad de umbrales** | Hardcoded | YAML configurable |

---

## üîÑ Plan de Implementaci√≥n (3-4 d√≠as)

### **D√≠a 1: Dashboard HTML**
- [ ] Implementar `DashboardGenerator.java`
- [ ] Dise√±o de layout HTML/CSS
- [ ] Integraci√≥n con Plotly.js
- [ ] 4 gr√°ficos principales
- [ ] Testing con datos reales de Sprint 1

### **D√≠a 2: Script Automatizado**
- [ ] Crear `run_quality_test.sh`
- [ ] Integrar todos los pasos del flujo
- [ ] Manejo de errores y logging
- [ ] Testing end-to-end

### **D√≠a 3: Sistema de Umbrales**
- [ ] Implementar `ThresholdChecker.java`
- [ ] Parser de `thresholds.yaml`
- [ ] L√≥gica de validaci√≥n por dimensi√≥n
- [ ] Output JSON de violaciones
- [ ] Exit codes para CI/CD

### **D√≠a 4: Documentaci√≥n y Validaci√≥n**
- [ ] Actualizar README.md
- [ ] Crear `validation-report.md`
- [ ] Crear `thresholds-guide.md`
- [ ] Screenshots del dashboard
- [ ] Testing final del flujo completo

---

## üéØ Entregables Sprint 3

1. **C√≥digo:**
   - `src/test/java/ssellm/DashboardGenerator.java`
   - `src/test/java/ssellm/ThresholdChecker.java`
   - `run_quality_test.sh`
   - `thresholds.yaml`

2. **Outputs:**
   - `target/quality_dashboard.html`
   - `target/threshold_report.json`

3. **Documentaci√≥n:**
   - `docs/sprint3/validation-report.md`
   - `docs/sprint3/thresholds-guide.md`
   - README.md actualizado

---

## üö® Riesgos y Mitigaciones

### Riesgo 1: Dashboard no renderiza bien en todos los navegadores
**Impacto**: Usuarios no pueden ver gr√°ficos
**Mitigaci√≥n**: Testing en Chrome, Firefox, Safari; usar Plotly.js (ampliamente compatible)

### Riesgo 2: Script automatizado falla por dependencias no instaladas
**Impacto**: Flujo no se puede ejecutar
**Mitigaci√≥n**: Validar dependencias al inicio del script, mensajes claros de error

### Riesgo 3: Umbrales muy estrictos generan falsos positivos
**Impacto**: CI/CD falla innecesariamente
**Mitigaci√≥n**: Calibrar umbrales con datos reales, permitir configuraci√≥n por entorno (dev/staging/prod)

---

## üí° Mejoras Futuras (Post-Sprint 3)

- **Alertas autom√°ticas**: Slack/Email cuando se violan umbrales
- **Dashboard en tiempo real**: WebSocket para actualizar durante el test
- **Comparaci√≥n hist√≥rica**: Guardar resultados y comparar con ejecuciones previas
- **Export a CSV/Excel**: Para an√°lisis offline

---

**Estado**: Planificado | **Owner**: Ricardo Campos | **√öltima actualizaci√≥n**: Octubre 2025
