package ssellm.analyzers;

import com.fasterxml.jackson.databind.JsonNode;
import ssellm.clients.OpenAIClient;
import ssellm.models.LLMJudgeEvaluation;
import ssellm.models.ResponseMetadata;

import java.util.ArrayList;
import java.util.List;
import java.util.stream.Collectors;

/**
 * LLM-as-a-judge evaluator using GPT-4 to assess response quality.
 * Sprint 2: Qualitative analysis to complement quantitative metrics.
 */
public class LLMJudge {

    private final OpenAIClient openAIClient;

    // System prompt for GPT-4 judge
    private static final String SYSTEM_PROMPT =
        "You are an expert evaluator of LLM responses. Your job is to analyze responses " +
        "generated by GPT-3.5-turbo and evaluate their quality according to objective criteria.\n\n" +
        "You MUST distinguish between:\n" +
        "- Legitimate variation (creativity, different valid approaches)\n" +
        "- Problematic inconsistency (technical errors, contradictions, incomplete responses)\n\n" +
        "Be fair and objective. Creative prompts SHOULD have varied responses. " +
        "Technical prompts SHOULD have consistent, correct responses.\n\n" +
        "You MUST respond with valid JSON only, no other text.";

    public LLMJudge(String apiKey) {
        this.openAIClient = new OpenAIClient(apiKey);
    }

    /**
     * Evaluate responses to a prompt using GPT-4 as judge
     *
     * @param prompt    Original prompt
     * @param category  Category of the prompt
     * @param responses List of ResponseMetadata objects
     * @return LLMJudgeEvaluation with scores and analysis
     */
    public LLMJudgeEvaluation evaluateResponses(String prompt, String category,
                                               List<ResponseMetadata> responses) {
        if (responses == null || responses.isEmpty()) {
            throw new IllegalArgumentException("Responses list cannot be null or empty");
        }

        System.out.println("\n‚öñÔ∏è GPT-4 Judge evaluating prompt: " +
            prompt.substring(0, Math.min(60, prompt.length())) + "...");
        System.out.println("   Category: " + category);
        System.out.println("   Responses to evaluate: " + responses.size());

        try {
            // Sample responses (max 5 for cost efficiency)
            List<ResponseMetadata> sampleResponses = sampleResponses(responses, 5);
            System.out.println("   üìä Sampling " + sampleResponses.size() + " responses for evaluation");

            // Build evaluation prompt
            String userPrompt = buildEvaluationPrompt(prompt, category, sampleResponses);

            // Call GPT-4 with JSON mode
            System.out.println("   ü§ñ Calling GPT-4 for evaluation...");
            JsonNode evaluation = openAIClient.evaluateWithGPT4JSON(SYSTEM_PROMPT, userPrompt);

            // Parse evaluation
            double similarityScore = evaluation.has("similarity_score") ?
                evaluation.get("similarity_score").asDouble() : 0.0;
            double technicalCorrectness = evaluation.has("technical_correctness") ?
                evaluation.get("technical_correctness").asDouble() : 0.0;
            double coherenceScore = evaluation.has("coherence_score") ?
                evaluation.get("coherence_score").asDouble() : 0.0;
            boolean creativityExpected = evaluation.has("creativity_expected") &&
                evaluation.get("creativity_expected").asBoolean();

            List<String> issuesDetected = parseJsonArray(evaluation.get("issues_detected"));
            List<String> legitimateVariations = parseJsonArray(evaluation.get("legitimate_variations"));

            System.out.println("   ‚úÖ Evaluation complete:");
            System.out.println("      Similarity: " + String.format("%.1f/10", similarityScore));
            System.out.println("      Technical Correctness: " + String.format("%.1f/10", technicalCorrectness));
            System.out.println("      Coherence: " + String.format("%.1f/10", coherenceScore));
            System.out.println("      Creativity Expected: " + (creativityExpected ? "Yes" : "No"));
            System.out.println("      Issues Detected: " + issuesDetected.size());
            System.out.println("      Legitimate Variations: " + legitimateVariations.size());

            // Build result
            LLMJudgeEvaluation result = LLMJudgeEvaluation.builder()
                .prompt(prompt)
                .category(category)
                .responseCount(sampleResponses.size())
                .similarityScore(similarityScore)
                .technicalCorrectness(technicalCorrectness)
                .coherenceScore(coherenceScore)
                .creativityExpected(creativityExpected)
                .issuesDetected(issuesDetected)
                .legitimateVariations(legitimateVariations)
                .rawLLMResponse(evaluation.toString())
                .build();

            double overallScore = result.getOverallScore();
            System.out.println("   üéØ Overall Score: " + String.format("%.2f/10", overallScore) +
                (overallScore >= 7.0 ? " ‚úÖ" : " ‚ö†Ô∏è"));

            return result;

        } catch (Exception e) {
            System.err.println("   ‚ùå Error in LLM judge evaluation: " + e.getMessage());
            e.printStackTrace();
            return createErrorResult(prompt, category, responses.size());
        }
    }

    /**
     * Build evaluation prompt for GPT-4
     */
    private String buildEvaluationPrompt(String originalPrompt, String category,
                                        List<ResponseMetadata> responses) {
        StringBuilder prompt = new StringBuilder();

        prompt.append("Analyze the following responses to a prompt and evaluate their quality.\n\n");
        prompt.append("**Original Prompt:**\n");
        prompt.append(originalPrompt).append("\n\n");
        prompt.append("**Category:** ").append(category).append("\n\n");
        prompt.append("**Responses to evaluate:**\n\n");

        for (int i = 0; i < responses.size(); i++) {
            ResponseMetadata metadata = responses.get(i);
            prompt.append("--- Response ").append(i + 1).append(" ---\n");
            prompt.append(metadata.getResponse()).append("\n\n");
        }

        prompt.append("**Evaluation Criteria:**\n\n");
        prompt.append("1. **Similarity Score (0-10)**: Do responses communicate the same core message?\n");
        prompt.append("   - 10: Identical in meaning\n");
        prompt.append("   - 7-9: Style variations but same content\n");
        prompt.append("   - 4-6: Some content differences\n");
        prompt.append("   - 0-3: Significantly different content\n\n");

        prompt.append("2. **Technical Correctness (0-10)**: Are responses technically accurate?\n");
        prompt.append("   - Validate code, concepts, terminology\n");
        prompt.append("   - Check for hallucinations or false information\n\n");

        prompt.append("3. **Coherence Score (0-10)**: Are responses logical and complete?\n");
        prompt.append("   - Check for incomplete thoughts\n");
        prompt.append("   - Verify logical structure\n\n");

        prompt.append("4. **Creativity Expected (boolean)**: Is variation expected for this type of prompt?\n");
        prompt.append("   - true: Creative/opinion-based prompts (different answers are valid)\n");
        prompt.append("   - false: Technical/factual prompts (should be consistent)\n\n");

        prompt.append("**Important:**\n");
        prompt.append("- For creative prompts (names, slogans, opinions): variation is GOOD, not a problem\n");
        prompt.append("- For technical prompts (code, facts, procedures): variation may indicate inconsistency\n");
        prompt.append("- Distinguish between 'different valid approaches' vs 'contradictory information'\n\n");

        prompt.append("Respond ONLY with valid JSON in this exact format:\n");
        prompt.append("{\n");
        prompt.append("  \"similarity_score\": <number 0-10>,\n");
        prompt.append("  \"technical_correctness\": <number 0-10>,\n");
        prompt.append("  \"coherence_score\": <number 0-10>,\n");
        prompt.append("  \"creativity_expected\": <boolean>,\n");
        prompt.append("  \"issues_detected\": [\"issue 1\", \"issue 2\"],\n");
        prompt.append("  \"legitimate_variations\": [\"variation 1\", \"variation 2\"]\n");
        prompt.append("}\n");

        return prompt.toString();
    }

    /**
     * Sample responses for evaluation (to reduce cost)
     */
    private List<ResponseMetadata> sampleResponses(List<ResponseMetadata> responses, int maxSamples) {
        // Only use complete responses
        List<ResponseMetadata> completeResponses = responses.stream()
            .filter(r -> !r.isTruncated() && r.getResponse() != null && !r.getResponse().isEmpty())
            .collect(Collectors.toList());

        if (completeResponses.size() <= maxSamples) {
            return completeResponses;
        }

        // Take evenly distributed samples
        List<ResponseMetadata> samples = new ArrayList<>();
        int step = completeResponses.size() / maxSamples;

        for (int i = 0; i < maxSamples && i * step < completeResponses.size(); i++) {
            samples.add(completeResponses.get(i * step));
        }

        return samples;
    }

    /**
     * Parse JSON array to List<String>
     */
    private List<String> parseJsonArray(JsonNode arrayNode) {
        List<String> result = new ArrayList<>();
        if (arrayNode != null && arrayNode.isArray()) {
            for (JsonNode item : arrayNode) {
                result.add(item.asText());
            }
        }
        return result;
    }

    /**
     * Create error result when evaluation fails
     */
    private LLMJudgeEvaluation createErrorResult(String prompt, String category, int responseCount) {
        return LLMJudgeEvaluation.builder()
            .prompt(prompt)
            .category(category)
            .responseCount(responseCount)
            .similarityScore(0.0)
            .technicalCorrectness(0.0)
            .coherenceScore(0.0)
            .creativityExpected(false)
            .issuesDetected(List.of("Evaluation failed - see logs"))
            .legitimateVariations(new ArrayList<>())
            .rawLLMResponse("{\"error\": \"Evaluation failed\"}")
            .build();
    }

    /**
     * Test the OpenAI connection
     */
    public boolean testConnection() {
        return openAIClient.testConnection();
    }
}
